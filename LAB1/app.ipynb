{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Folder containing the dataset\n",
    "data_folder = \"./pima-5-fold\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to parse KEEL format files\n",
    "def load_keel_file(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "    # Find the start of the data section\n",
    "    data_start = next(i for i, line in enumerate(lines) if \"@data\" in line.lower())\n",
    "    \n",
    "    # Extract data and load into a DataFrame\n",
    "    data = pd.read_csv(\n",
    "        file_path,\n",
    "        skiprows=data_start + 1,\n",
    "        header=None\n",
    "    )\n",
    "\n",
    "    # Extract column names from the header\n",
    "    attribute_lines = [line for line in lines if line.lower().startswith(\"@attribute\")]\n",
    "    column_names = [line.split()[1] for line in attribute_lines]\n",
    "\n",
    "    data.columns = column_names\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter tuning range for k\n",
    "k_values = [1, 3, 5, 7, 9, 11]\n",
    "\n",
    "# Dictionary to store results\n",
    "results = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over each k value\n",
    "for k in k_values:\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    metrics = {\n",
    "        'accuracy': [],\n",
    "        'precision': [],\n",
    "        'recall': [],\n",
    "        'f1_score': [],\n",
    "        'roc_auc': [],\n",
    "        'sensitivity': [],\n",
    "        'specificity': [],\n",
    "        'true_positive_rate': [],\n",
    "        'false_alarm_rate': []\n",
    "    }\n",
    "\n",
    "    # Perform evaluation across the 5 training-test pairs\n",
    "    for fold in range(1, 6):\n",
    "        # File paths for training and test data\n",
    "        train_file = os.path.join(data_folder, f\"pima-5-{fold}tra.dat\")\n",
    "        test_file = os.path.join(data_folder, f\"pima-5-{fold}tst.dat\")\n",
    "\n",
    "        # Load data\n",
    "        train_data = load_keel_file(train_file)\n",
    "        test_data = load_keel_file(test_file)\n",
    "\n",
    "        # Convert categorical target to numeric\n",
    "        le = LabelEncoder()\n",
    "        train_data['Class'] = le.fit_transform(train_data['Class'])\n",
    "        test_data['Class'] = le.transform(test_data['Class'])\n",
    "\n",
    "        # Split features and target\n",
    "        X_train, y_train = train_data.iloc[:, :-1], train_data.iloc[:, -1]\n",
    "        X_test, y_test = test_data.iloc[:, :-1], test_data.iloc[:, -1]\n",
    "\n",
    "        # Feature scaling\n",
    "        scaler = StandardScaler()\n",
    "        X_train = scaler.fit_transform(X_train)\n",
    "        X_test = scaler.transform(X_test)\n",
    "\n",
    "        # Train the model\n",
    "        knn.fit(X_train, y_train)\n",
    "\n",
    "        # Predict on the test set\n",
    "        y_pred = knn.predict(X_test)\n",
    "        y_prob = knn.predict_proba(X_test)[:, 1] if hasattr(knn, 'predict_proba') else None\n",
    "\n",
    "        # Compute confusion matrix\n",
    "        tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "\n",
    "        # Compute metrics\n",
    "        metrics['accuracy'].append(accuracy_score(y_test, y_pred))\n",
    "        metrics['precision'].append(precision_score(y_test, y_pred, zero_division=0))\n",
    "        metrics['recall'].append(recall_score(y_test, y_pred, zero_division=0))\n",
    "        metrics['f1_score'].append(f1_score(y_test, y_pred))\n",
    "        if y_prob is not None:\n",
    "            metrics['roc_auc'].append(roc_auc_score(y_test, y_prob))\n",
    "        metrics['sensitivity'].append(tp / (tp + fn) if (tp + fn) > 0 else 0)\n",
    "        metrics['specificity'].append(tn / (tn + fp) if (tn + fp) > 0 else 0)\n",
    "        metrics['true_positive_rate'].append(tp / (tp + fn) if (tp + fn) > 0 else 0)\n",
    "        metrics['false_alarm_rate'].append(fp / (fp + tn) if (fp + tn) > 0 else 0)\n",
    "\n",
    "    # Store average metrics for this k\n",
    "    results[k] = {metric: np.mean(values) for metric, values in metrics.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameter tuning results:\n",
      "\n",
      "k = 1\n",
      "  accuracy: 0.7070\n",
      "  precision: 0.5907\n",
      "  recall: 0.5222\n",
      "  f1_score: 0.5536\n",
      "  roc_auc: 0.6641\n",
      "  sensitivity: 0.5222\n",
      "  specificity: 0.8060\n",
      "  true_positive_rate: 0.5222\n",
      "  false_alarm_rate: 0.1940\n",
      "\n",
      "k = 3\n",
      "  accuracy: 0.7448\n",
      "  precision: 0.6549\n",
      "  recall: 0.5706\n",
      "  f1_score: 0.6083\n",
      "  roc_auc: 0.7680\n",
      "  sensitivity: 0.5706\n",
      "  specificity: 0.8380\n",
      "  true_positive_rate: 0.5706\n",
      "  false_alarm_rate: 0.1620\n",
      "\n",
      "k = 5\n",
      "  accuracy: 0.7422\n",
      "  precision: 0.6557\n",
      "  recall: 0.5483\n",
      "  f1_score: 0.5966\n",
      "  roc_auc: 0.7751\n",
      "  sensitivity: 0.5483\n",
      "  specificity: 0.8460\n",
      "  true_positive_rate: 0.5483\n",
      "  false_alarm_rate: 0.1540\n",
      "\n",
      "k = 7\n",
      "  accuracy: 0.7461\n",
      "  precision: 0.6601\n",
      "  recall: 0.5519\n",
      "  f1_score: 0.5997\n",
      "  roc_auc: 0.7928\n",
      "  sensitivity: 0.5519\n",
      "  specificity: 0.8500\n",
      "  true_positive_rate: 0.5519\n",
      "  false_alarm_rate: 0.1500\n",
      "\n",
      "k = 9\n",
      "  accuracy: 0.7408\n",
      "  precision: 0.6612\n",
      "  recall: 0.5257\n",
      "  f1_score: 0.5838\n",
      "  roc_auc: 0.7943\n",
      "  sensitivity: 0.5257\n",
      "  specificity: 0.8560\n",
      "  true_positive_rate: 0.5257\n",
      "  false_alarm_rate: 0.1440\n",
      "\n",
      "k = 11\n",
      "  accuracy: 0.7499\n",
      "  precision: 0.6742\n",
      "  recall: 0.5483\n",
      "  f1_score: 0.6034\n",
      "  roc_auc: 0.7962\n",
      "  sensitivity: 0.5483\n",
      "  specificity: 0.8580\n",
      "  true_positive_rate: 0.5483\n",
      "  false_alarm_rate: 0.1420\n",
      "\n",
      "Best k based on accuracy: 11\n",
      "Metrics for best k (11): {'accuracy': 0.7499448264154147, 'precision': 0.6741987256065003, 'recall': 0.5482879105520615, 'f1_score': 0.603430324504581, 'roc_auc': 0.7962438853948288, 'sensitivity': 0.5482879105520615, 'specificity': 0.858, 'true_positive_rate': 0.5482879105520615, 'false_alarm_rate': 0.14200000000000002}\n"
     ]
    }
   ],
   "source": [
    "# Print results\n",
    "print(\"Hyperparameter tuning results:\\n\")\n",
    "for k, metrics in results.items():\n",
    "    print(f\"k = {k}\")\n",
    "    for metric, value in metrics.items():\n",
    "        print(f\"  {metric}: {value:.4f}\")\n",
    "    print()\n",
    "\n",
    "# Find the best k based on accuracy\n",
    "best_k = max(results, key=lambda x: results[x]['accuracy'])\n",
    "print(f\"Best k based on accuracy: {best_k}\")\n",
    "print(f\"Metrics for best k ({best_k}): {results[best_k]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
